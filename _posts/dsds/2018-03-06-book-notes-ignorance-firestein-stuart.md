---
title: "Book Notes: Ignorance -Firestein, Stuart"
date: "2018-03-06"
categories: 
  - "research"
---

- “It is very difficult to find a black cat in a dark room,” warns an old proverb. “Especially when there is no cat.”
- Andrew Wiles describes it: It’s groping and probing and poking, and some bumbling and bungling, and then a switch is discovered, often by accident, and the light is lit, and everyone says, “Oh, wow, so that’s how it
- Now I use the word ignoranc e at least in part to be intentionally provocative. But let’s take a moment to define the kind of ignorance I am referring to, because ignorance has many bad connotations, especially in common usage, and I don’t mean any of those. One kind of ignorance is willful stupidity; worse than simple stupidity, it is a callow indifference to facts or logic. It shows itself as a stubborn devotion to uninformed opinions, ignoring (same root) contrary ideas, opinions, or data. The ignorant are unaware, unenlightened, uninformed, and surprisingly often occupy elected offices. We can all agree that none of this is good. But there is another, less pejorative sense of ignorance that describes a particular condition of knowledge: the absence of fact, understanding, insight, or clarity about something. It is not an individual lack of information but a communal gap in knowledge. It is a case where data don’t exist, or more commonly, where the existing data don’t make sense, don’t add up to a coherent explanation, cannot be used to make a prediction or statement about some thing or event. This is knowledgeable ignorance, perceptive ignorance, insightful ignorance. It leads us to frame better questions, the first step to getting better answers. It is the most important resource we scientists have, and using it correctly is the most important thing a scientist does. James Clerk Maxwell, perhaps the greatest physicist between Newton and Einstein, advises that “Thoroughly conscious ignorance is the prelude to every real advance in science.”
- I have learned from years of teaching that saying nearly the same thing in different ways is an often effective strategy. Sometimes a person has to hear something a few times or just the right way to get that click of recognition, that “ah-ha moment” of clarity.
- Questions are bigger than answers. One good question can give rise to several layers of answers, can inspire decades-long searches for solutions, can generate whole new fields of inquiry, and can prompt changes in entrenched thinking. Answers,on the other hand, often end the process.
- Working scientists don’t get bogged down in the factual swamp because they don’t care all that much for facts. It’s not that they discount or ignore them, but rather that they don’t see them as an end in themselves. They don’t stop at the facts; they begin there, right beyond the facts, where the facts run out. Facts are selected, by a process that is a kind of controlled neglect, for the questions they create, for the ignorance they point to. What if we cultivated ignorance instead of fearing it, what if we controlled neglect instead of feeling guilty about it, what if we the power of not knowing in a world dominated by information? As the first philosopher, Socrates, said, “I know one thing, that I know nothing.” and knowing lots of them does not automatically make you a scientist, just a geek.
- As a scientist, you don’t do something with what you know to defend someone, treat someone, or make someone a pile of money. You use those facts to frame a new question—to speculate about a new black cat. In other words, scientists don’t concentrate on what they know, which is considerable but also miniscule, but rather on what they don’t know. The one big fact is that science traffics in ignorance, cultivates it, and is driven by it.
- The poet John Keats hit upon an ideal state of mind for the literary psyche that he called Negative Capability—“that is when a man is capable of being in uncertainties, Mysteries, doubts without any irritable reaching after fact & reason.” He considered Shakespeare to be the exemplar of this state of mind, allowing him to inhabit the thoughts and feelings of his characters because his imagination was not hindered by certainty, fact, and mundane reality (think Hamlet). This notion can be adapted to the scientist who really should always find himself or herself in this state of “uncertainty without irritability.”
- Erwin Schrodinger, one of the great philosopher-scientists, says, “In an honest search for knowledge you quite often have to abide by ignorance for an indefinite period.”
- Schrodinger knew something about uncertainty; he posed the now famous Schrodinger’s cat thought experiment in which a cat placed in a box with a vial of poison that could or could not be according to some quantum event was, until observed, both dead and alive, or neither.)
- Max Planck, the brilliant physicist who led the revolution in physics now known as quantum mechanics, was asked how often science changed. He replied: “with every funeral,” a nod to the way science often changes on a generational time scale. It is now clear that a large bump on the right side of your head just behind your ear has nothing to do with your being an especially combative person. Nonetheless, hundreds of scientific papers appeared in the literature, and several highly respected scientific names of the 19th century were attached to it.
- Almost everyone believes that the tongue has regional sensitivities—sweet is sensed on the tip, bitter on the back, salt and sour on the sides. Pictures of “tongue maps” continue to appear not only in popular books on taste and cooking but medical textbooks as well. The only problem is that it’s not true. The whole thing arose from the mistranslation of a German physiology textbook by a Professor D. P. Hanig, who claimed that his very anecdotal experiments showed that parts of the tongue were slightly more or slightly less sensitive to the four basic tastes. Very slightly as it has turned out when the experiments are done more carefully (you can try this on your own tongue with some salt and sugar, for example). The Hanig work was published in 1901 and the translation, which considerably overstated the findings and canonized the myth, was by the famed Harvard psychologist Edward G. Boring (joke to thousands of undergraduate psychology majors forced to read his textbooks) in 1942. Boring, by the way, was a pioneer in sensory perception who gave us the well-known ambiguous figure that can be seen, depending on how you look at it, as a beautiful young woman or an old hag. Perhaps at least in part because of Boring’s stature the mythical tongue map was canonized into a fact and maintained by repetition, rather than experimentation, now having endured for more than a century.
- These spikes have since been recorded in virtually every area of the brain and in all the sensory organs, and they have come to be regarded as the language of the brain—that is, they encode all the information passing into and around the brain. Spikes are a fundamental unit of neurobiology. For the last 75 years my neuroscience colleagues and I have been studying spikes and teaching our students about spikes and making grand theories about how the brain works based on spiking behavior. Some of it is true. But what have we missed by concentrating on spikes for the last eight decades? A lot, it turns out. There are many other electrical sorts of signals in the brain, not as prominent as spikes, but that’s a reflection of our recording technology not of the brain itself. These other processes, as well as chemical events that are not electrical, and therefore can’t even be seen with an electrical apparatus, are now being recognized as perhaps the more salient features of brain activity.
- But we have been mesmerized by spikes, and the rest has been virtually invisible, even though it is right in front of our faces, happening all the time in our
- For at least as long as I have been teaching neuroscience, I have told students that the human brain is composed of about 100 billion neurons and 10 times that number of glial cells—a kind of cell that nourishes neurons and provides some packing and structure for the organ (the word glia comes from Greek for “glue”). These numbers are also in all the textbooks. In early 2009 I received an e-mail from a neuroanato-mist in Argentina named Suzana Herculano-Houzel asking me if I would help her group’s research project by taking a short survey. Among the questions on that survey was how many neurons and glial cells I thought were in the human brain, and where I got that number from. The first part of the question was easy—I filled in the stock answers. But actually I wasn’t sure where that number had come from. It was in the textbooks, but no references to any work were ever given for it. No one, it turned out, knew where the number came from. It sounded reasonable; it wasn’t, after all, an exact number, not like 101,954,467,298 neurons, which would have required a reference to back it up. A little over a year later I heard back from Suzana. Her group had developed a new method for counting cells that was more exact and less prone to errors and could be used on big tissues, like brains. They counted the number of neurons and the number of glial cells in several human brains. For neurons they found that the average number for humans is 86 billion—20% less than we thought; and more remarkably for glial cells there were about an equal number as there were neurons—not 10 times
- Seems we don’t always know what we don’t know.
- physicist Werner Heisenberg’s Uncertainty Principle tells us that in the subatomic universe there are theoretically imposed limits to knowledge—the position and momentum of a subatomic particle (as well as other pairs of observations) can never be known simultaneously. Similarly, in mathematics, Kurt Gödel in his Incompleteness Theorems demonstrated that every logical system that is complex enough to be interesting must remain incomplete. Are there other limits like these? For example, in biology some ask whether a brain can understand itself. Turbulence or the weather may be fundamentally unpredictable in ways we don’t yet grasp.
- Some fundamental things can never be known with certainty.
- The universe is not deterministic; it is probabilistic, and the future can’t be predicted with certainty. Now it is true that, as a practical matter, for things that have masses greater than about 10 –28 grams, the probabilities become so large that predicting how they will act is quite possible—baseball players regularly predict the path of 150 gram spheres on a trajectory covering 100 meters and if a shoe that has been thrown by an irate journalist is coming at your head from the right, ducking to the left is certainly a good bet.
- The most famous of these is the Cretan’s paradox, sometimes known as the Liar’s paradox. They all go something like: “The Cretan claims that all Cretans are liars.” So who are you to believe? Or another version—take a blank card and write on one side of it, “The statement on the other side of this card is true” and on the other side write, “The statement on the other side of this card is false.” These little mind games, became for Gödel, the basis of a new form of logic that he used to demonstrate that in many circumstances you can’t tell yourself the truth.
- It is that the problem of the unknowable, even the really unknowable, may not be a serious obstacle. The unknowable may itself become a fact. It can serve as a portal to deeper understanding.
- Things never go the way we think they will; there are always unexpected findings and unexpected consequences that may redirect or even stymie a field for years.
